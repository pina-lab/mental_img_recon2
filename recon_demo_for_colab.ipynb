{"cells":[{"cell_type":"markdown","metadata":{"id":"hkyJQ3cWp7Fo"},"source":["# Paper\n","This is the demo code for \"Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based Bayesian estimation\""]},{"cell_type":"markdown","metadata":{"id":"OTk-KHrHqIiC"},"source":["## Setup\n","Clone the repository and download decoded features and pretrained VQGANs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zax-ZMC7qZB_"},"outputs":[],"source":["# Clone the repository\n","# TODO: URL を設定します。\n","!git clone XXXXX\n","%cd mental-image-reconstruction\n","\n","# Display the eyecatch_image\n","from PIL import Image\n","eyecatch_path = './ref_images/eyecatch__mental_im_recon__+_x_ver.png'\n","Image.open(eyecatch_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":52368,"status":"ok","timestamp":1718781839088,"user":{"displayName":"Naoki Takada","userId":"14996079913833707807"},"user_tz":-540},"id":"rMNfluf3kK_L","outputId":"76c696e4-1ac5-49be-c000-4a3f46ee00ac"},"outputs":[],"source":["# Download the feature data from Google Drive\n","import gdown\n","import tarfile\n","\n","# Set the public file ID\n","# TODO: ファイル ID を変更します。\n","file_id = 'XXXX'\n","download_url = f'https://drive.google.com/uc?id={file_id}'\n","download_path = '/content/mental-image-reconstruction/downloaded_file.tar.gz'\n","# Download the tar.gz file\n","gdown.download(download_url, download_path, quiet=False)\n","\n","# Extract the downloaded tar.gz file\n","with tarfile.open(download_path, 'r:gz') as tar:\n","    tar.extractall(path='/content/mental-image-reconstruction')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":79787,"status":"ok","timestamp":1718631425934,"user":{"displayName":"Ryoga Otake","userId":"02575859105280299768"},"user_tz":-540},"id":"2PV7eAWQZvpy","outputId":"52737498-6caf-47cc-f856-5c6f23b74b60"},"outputs":[],"source":["# Install VQGAN\n","!git clone https://github.com/CompVis/taming-transformers\n","%cd taming-transformers\n","!mkdir -p logs/vqgan_imagenet_f16_1024/checkpoints\n","!mkdir -p logs/vqgan_imagenet_f16_1024/configs\n","!wget 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt'\n","!wget 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/configs/model.yaml'\n","# Modify part of the code to be compatible with PyTorch 2.x\n","!sed -i 's/from torch._six import string_classes/string_classes = str/' ./taming/data/utils.py\n","%pip install -e .\n","%cd .."]},{"cell_type":"markdown","metadata":{"id":"cnbeLhgiq1Pn"},"source":["Install minimal required dependencies."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":23810,"status":"ok","timestamp":1718681683855,"user":{"displayName":"Ryoga Otake","userId":"02575859105280299768"},"user_tz":-540},"id":"DiaPV1_BaGcs","outputId":"78596efe-40e3-411d-8203-d8e7618d5fbf"},"outputs":[],"source":["%pip install mat73 omegaconf einops ftfy regex tqdm pytorch-lightning\n","%pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFDZyEImrVE2"},"outputs":[],"source":["import torch\n","import sys\n","import yaml\n","\n","# Load demo params\n","with open('demo_params.yaml', 'rb') as f:  # config\n","    prm_demo = yaml.safe_load(f)\n","# Load config for imrecon\n","with open('config.yaml', 'rb') as f:  # config\n","    dt_cfg = yaml.safe_load(f)\n","\n","# Set directory of taming_transformer\n","dir_taming_transformer = dt_cfg['file_path']['taming_transformer_dir']\n","sys.path.insert(0, dir_taming_transformer)\n","\n","# Import required modules\n","\n","# Set GPU if it's available\n","cudaID = \"cuda:0\"\n","DEVICE = torch.device(cudaID if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"9EIXKHYgroCw"},"source":["## Load the VQGANs, VGG19 and CLIP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7YTbMqqridy"},"outputs":[],"source":["import model_loading\n","\n","# load VQGAN model\n","config1024 = model_loading.load_config(\n","    dir_taming_transformer+\"/logs/vqgan_imagenet_f16_1024/configs/model.yaml\", display=False)\n","VQGANmodel1024 = model_loading.load_vqgan(\n","    config1024, ckpt_path=dir_taming_transformer+\"/logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\").to(DEVICE)\n","VQGANmodel1024.eval()\n","\n","# Load VGG19 model\n","VGGmodel_, _ = model_loading.load_VGG_model(DEVICE)\n","\n","# Load CLIP models to be used.\n","# set CLIPmodelName_\n","CLIP_modelNames = dt_cfg[\"models\"][\"CLIP\"][\"modelnames\"]\n","CLIP_modelTypes = dt_cfg[\"models\"][\"CLIP\"][\"modeltypes\"]\n","CLIP_usedLayer = dt_cfg[\"models\"][\"CLIP\"][\"used_layer\"]\n","CLIPmodelWeight_ = dt_cfg[\"models\"][\"CLIP\"][\"modelcoefs\"]\n","CLIPmodel_, nameOfSubdirForCLIPfeature = model_loading.load_CLIP_model(\n","    CLIP_modelTypes, DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"5iQ2S5R7sJPu"},"source":["## Set parameters\n","Set parameters for image reconstruction.\n","Select the subject (S01, S02, S03), target image for reconstruction (0 ~ 24), and reconstruction method (original, Langevin, withoutLangevin, withoutVQGAN)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32cztdrbr_cM"},"outputs":[],"source":["subject = 'S02'  # select from 'S01', 'S02', 'S03'\n","\n","targetID_list = [21, 20, 18, 19, 7, 14]  # select from 0 to 24\n","\n","# select from 'original' (default), 'Langevin', 'withoutLangevin', 'withoutVQGAN'\n","reconMethod = 'original'"]},{"cell_type":"markdown","metadata":{"id":"VhV0BGl_o52e"},"source":["# Reconstruction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvFwZfpvY81U"},"outputs":[],"source":["# start reconstruction\n","import recon_utils as utils\n","utils.start_reconstruction(\n","    subject, targetID_list, reconMethod, dt_cfg, prm_demo,\n","    CLIPmodel_, VGGmodel_, CLIPmodelWeight_, VQGANmodel1024, DEVICE)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
